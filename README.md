
# ğŸ” Dual-Layer Prompt Injection Defense for LLMs

A cutting-edge security framework for mitigating **prompt injection attacks** on chat-based Large Language Models (LLMs). This dual-layered defense system combines an **auxiliary prompt classifier** with a **LoRA-fine-tuned LLM**, offering high recall for malicious prompt detection and robust in-model behavior alignment.

---

## ğŸ§  Project Description

This project is designed to provide production-grade protection against **prompt injection**, **jailbreak-style attacks**, and **role hijacking** in LLM applications. It works as follows:

1. **Prompt Classifier (Layer 1):**  
   A lightweight transformer model (e.g., DistilBERT/DeBERTa) classifies user input as either _malicious_ or _benign_ before it reaches the LLM.

2. **Injection-Resistant LLM (Layer 2):**  
   A powerful open-weight chat model (e.g., LLaMA2-Chat or Mistral-Instruct) fine-tuned via LoRA/QLoRA on adversarial and aligned prompt-response pairs.

3. **Feedback Loop & Continuous Learning:**  
   Logged data feeds a retraining pipeline for updating the classifier and LLM adapters, creating a self-healing security system.

---

## ğŸ—‚ï¸ Project Structure

```
dual-layer-defense/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train_prompts.csv          # Training data for classifier
â”‚   â”œâ”€â”€ lora_finetune_dataset.json # Prompt-response pairs for LLM fine-tuning
â”‚   â””â”€â”€ eval_set.csv               # Evaluation prompts (malicious & benign)
â”‚
â”œâ”€â”€ classifier/
â”‚   â”œâ”€â”€ train_classifier.py        # Classifier fine-tuning script (HuggingFace)
â”‚   â”œâ”€â”€ inference.py               # Runtime prompt classification logic
â”‚   â””â”€â”€ model/                     # Saved classifier model & tokenizer
â”‚
â”œâ”€â”€ lora_finetune/
â”‚   â”œâ”€â”€ finetune_lora.py           # Fine-tuning LLM with LoRA/QLoRA
â”‚   â””â”€â”€ adapter/                   # Trained LoRA adapter weights
â”‚
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ main.py                    # FastAPI app: integrates classifier + LLM
â”‚   â””â”€â”€ middleware.py              # Input sanitation, output logging, etc.
â”‚
â”œâ”€â”€ monitor/
â”‚   â”œâ”€â”€ log_events.jsonl           # Inference logs for analysis
â”‚   â””â”€â”€ feedback_loop.py           # Training loop from logged attacks
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## ğŸš€ How to Run

### 1. Setup Environment

```bash
git clone https://github.com/yourusername/dual-layer-defense.git
cd dual-layer-defense
pip install -r requirements.txt
```

### 2. Train the Classifier

```bash
python classifier/train_classifier.py
```

### 3. Fine-Tune the LLM using LoRA

```bash
python lora_finetune/finetune_lora.py
```

### 4. Launch the Chat API

```bash
uvicorn api.main:app --reload
```

---

## ğŸ” Evaluation Metrics

| Component   | Metric     | Target Value |
|-------------|------------|---------------|
| Classifier  | Recall     | â‰¥ 0.98        |
| Classifier  | Precision  | â‰¥ 0.90        |
| LLM Output  | ASR        | â‰¤ 0.01        |
| System      | Latency    | < 200ms avg   |

---

## ğŸ§ª Test Adversarial Prompts

You can evaluate your system using a suite of adversarial prompts:

```bash
python monitor/test_prompts.py --input data/eval_set.csv
```

---

## ğŸ’¡ Key Technologies

- ğŸ¤— Transformers (HuggingFace)
- PEFT (LoRA/QLoRA)
- DeBERTa / DistilBERT
- LLaMA2 / Mistral (7B/13B)
- FastAPI
- PyTorch

---

## ğŸ“Œ Future Work

- Multilingual classifier support
- Prompt fingerprinting/token watermarking
- Context-aware classifier (multi-turn)
- Output classifier for toxic/unsafe completions

---

## ğŸ“œ License

MIT License Â© 2025

---

## ğŸ¤ Contributing

Pull requests are welcome. For major changes, please open an issue first to discuss what you'd like to change or add.

---

## ğŸ“« Contact

For questions or collaboration inquiries, reach out at: [your-email@example.com]
