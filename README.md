
# ðŸ›¡ï¸ PromptInjectionDefense: Dual-Layer Defense Against Prompt Injection Attacks on LLMs

Welcome to the official repository for **PromptInjectionDefense**, a production-grade, research-oriented system designed to secure Large Language Models (LLMs) from prompt injection and jailbreak-style attacks.

This repository implements a **dual-layer safeguard system** combining:
1. **Auxiliary Classifier** â€“ Pre-filters incoming prompts to detect malicious intent using a transformer-based model (e.g., DeBERTa, DistilBERT).
2. **Fine-Tuned LLM with LoRA** â€“ A robust instruction-following LLM (e.g., LLaMA2-Chat or Mistral-Instruct), enhanced using parameter-efficient fine-tuning (LoRA/QLoRA) on adversarial and benign prompts.

---

## ðŸ”§ Project Structure

```
PromptInjection/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train_prompts.csv
â”‚   â”œâ”€â”€ test_prompts.csv
â”‚   â””â”€â”€ logs/
â”œâ”€â”€ classifier/
â”‚   â”œâ”€â”€ train_classifier.py
â”‚   â””â”€â”€ model/
â”œâ”€â”€ lora_llm/
â”‚   â”œâ”€â”€ train_lora.py
â”‚   â””â”€â”€ adapter/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ server.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ eval_classifier.py
â”‚   â”œâ”€â”€ eval_llm.py
â”‚   â””â”€â”€ benchmark_prompts/
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ data_preparation.ipynb
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## ðŸ§  System Architecture

```mermaid
flowchart TD
    A[User Input] --> B{Prompt Classifier}
    B -- Malicious --> C[Block / Refusal Message]
    B -- Safe --> D[LLM (LoRA Tuned)]
    D --> E[Secure Output]
    E --> F[Logging & Feedback]
```

> The classifier is your first line of defense. The LLM is hardened to ignore injected content. Logging enables future retraining and continuous adaptation.

---

## ðŸš€ Quickstart

1. **Clone the repo**
```bash
git clone https://github.com/yourusername/PromptInjection.git
cd PromptInjection
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Train classifier**
```bash
python classifier/train_classifier.py
```

4. **Fine-tune LLM with LoRA**
```bash
python lora_llm/train_lora.py
```

5. **Run the API**
```bash
python api/server.py
```

---

## ðŸ“Š Evaluation

- Evaluate classifier: `python evaluation/eval_classifier.py`
- Evaluate LLM robustness: `python evaluation/eval_llm.py`

Metrics: Precision, Recall, F1-score, Attack Success Rate (ASR)

---

## ðŸ“ˆ Continuous Improvement

- New adversarial prompts â†’ label â†’ add to dataset
- Retrain LoRA adapters weekly
- Use monitoring logs for human-in-the-loop feedback

---

## ðŸ§© Future Work

- Add multilingual support
- Integrate with vector DB for prompt similarity detection
- Research multi-modal prompt injection (e.g., image + text)

---

## ðŸ“„ License

Apache 2.0. Free for research and commercial use.
